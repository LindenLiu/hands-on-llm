{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install umap-learn\n",
    "# !pip install hdbscan\n",
    "# !pip install seaborn\n",
    "# !pip install bertopic\n",
    "# !pip install nbformat\n",
    "# !pip install 'bertopic[spacy]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/linden/Workspace/learn_llm/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('maartengr/arxiv_nlp')['train']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract specific metadata\n",
    "abstracts = dataset[\"Abstracts\"]\n",
    "years = dataset[\"Years\"]\n",
    "categories = dataset[\"Categories\"]\n",
    "titles = dataset[\"Titles\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A typic cluster pipeline is \n",
    "\n",
    "1. Embeddocuments\n",
    "2. Reducedimensionality \n",
    "3. Clusterembeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = model.encode(abstracts, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we cluster the embeddings we generated from the ArXiv abstracts, we need to take care of the curse of dimensionality first. This curse is a phenomenon that occurs when dealing with high-dimensional data. As the number of dimensions increases, there is an exponential growth of the number of possible values within each dimension. Finding all subspaces within each dimension becomes increasingly complex. Moreover, as the number of dimensions grows, the concept of distance between points becomes increasingly less precise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from umap import UMAP\n",
    "\n",
    "umap_model = UMAP(n_neighbors=15, n_components=5, metric='cosine')\n",
    "\n",
    "reduced_embeddings = umap_model.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdbscan import HDBSCAN\n",
    "\n",
    "hdbscan_model = HDBSCAN(min_cluster_size=15, min_samples=1, metric='euclidean', cluster_selection_method='eom')\n",
    "hdbscan_model.fit(reduced_embeddings)\n",
    "labels = hdbscan_model.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "reduced_embeddings = UMAP(n_neighbors=15, n_components=2, metric='cosine').fit_transform(embeddings)\n",
    "df = pd.DataFrame(np.hstack((reduced_embeddings, labels.reshape(-1, 1))), columns=[\"x\", \"y\", \"cluster\"])\n",
    "df.cluster = df.cluster.astype(int).astype(str)\n",
    "sns.scatterplot(data=df, x='x', y='y', hue='cluster', linewidth=0, legend=False, s=3, alpha=0.3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index in np.where(labels==1)[0][:3]:\n",
    "    print(abstracts[index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic Models \n",
    "Latent Dirichlet Allocation (LDA; blei2003latent) is a classical and popular approach to topic modeling that assumes that each topic is characterized by a probability distribution over words in a corpus vocabulary. Each document is to be considered a mixture of topics\n",
    "\n",
    "A bit morden way is BERTopic which is a pipeline like follows\n",
    "Sentence BERT -> UMAP -> HDSCAN -> CountVector -> c-IDF-TF \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "666.38s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "666.39s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "666.40s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "666.43s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "666.45s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n",
      "0.00s - Debugger warning: It seems that frozen modules are being used, which may\n",
      "0.00s - make the debugger miss breakpoints. Please pass -Xfrozen_modules=off\n",
      "0.00s - to python to disable frozen modules.\n",
      "0.00s - Note: Debugging will proceed. Set PYDEVD_DISABLE_FILE_VALIDATION=1 to disable this validation.\n"
     ]
    }
   ],
   "source": [
    "from bertopic import BERTopic\n",
    "\n",
    "topic_model = BERTopic()\n",
    "topics, probabilities = topic_model.fit_transform(documents=abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The standard bertopic implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.visualize_documents(docs=titles, topics=[0, 1, 2, 3, 4, 6, 7, 10, 12,13, 16, 33, 40, 45, 46, 65])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.get_topic_info(topics[titles.index(\"Linguistic Information Energy\")])\n",
    "abstracts[titles.index(\"Linguistic Information Energy\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "original_topics = deepcopy(topic_model.topic_representations_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topic_differences(topic_model, original_topics, max_length=60):\n",
    "    topic_number = len(original_topics) - 1\n",
    "    for topic in range(topic_number):\n",
    "        og_words = \" | \".join(list(zip(*original_topics[topic][:5]))[0])\n",
    "        new_words = \" | \".join(list(zip(*topic_model.topic_representations_[topic][:5]))[0])\n",
    "        space = \" \" * max(0, (max_length - len(og_words)))\n",
    "        print(f\"Topic {topic}: {og_words}{space} -> {new_words}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation import KeyBERTInspired\n",
    "\n",
    "key_bert_inspired_model = KeyBERTInspired()\n",
    "key_bert_model = BERTopic(representation_model=key_bert_inspired_model)\n",
    "key_bert_model.fit_transform(documents=abstracts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_differences(key_bert_model, original_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_differences(key_bert_model, original_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bertopic.representation._pos import PartOfSpeech\n",
    "pos = PartOfSpeech(\"en_core_web_sm\")\n",
    "\n",
    "key_bert_model.update_topics(abstracts, representation_model=pos)\n",
    "topic_differences(key_bert_model, original_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Maximal Marginal Relevance\n",
    "from bertopic.representation import MaximalMarginalRelevance\n",
    "\n",
    "mmr_model = MaximalMarginalRelevance(diversity=0.5)\n",
    "topic_model.update_topics(abstracts, representation_model=mmr_model)\n",
    "\n",
    "topic_differences(topic_model, original_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "I have topic that contains the following documents: \\n[DOCUMENTS]\n",
    "The topic is described by the following keywords: [KEYWORDS]\n",
    "\n",
    "Based on the above information, can you give a short label of the topic?\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1216 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m text_generator \u001b[38;5;241m=\u001b[39m pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext2text-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/flan-t5-base\u001b[39m\u001b[38;5;124m\"\u001b[39m, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m      6\u001b[0m representation_model \u001b[38;5;241m=\u001b[39m TextGeneration(text_generator, prompt\u001b[38;5;241m=\u001b[39mprompt)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mtopic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mabstracts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrepresentation_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrepresentation_model\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m topic_differences(topic_model, original_topics)\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/bertopic/_bertopic.py:1428\u001b[0m, in \u001b[0;36mBERTopic.update_topics\u001b[0;34m(self, docs, images, topics, top_n_words, n_gram_range, vectorizer_model, ctfidf_model, representation_model)\u001b[0m\n\u001b[1;32m   1426\u001b[0m documents_per_topic \u001b[38;5;241m=\u001b[39m documents\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTopic\u001b[39m\u001b[38;5;124m'\u001b[39m], as_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39magg({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDocument\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin})\n\u001b[1;32m   1427\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mc_tf_idf_, words \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_c_tf_idf(documents_per_topic)\n\u001b[0;32m-> 1428\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopic_representations_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_extract_words_per_topic\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1429\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mset\u001b[39m(topics) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtopics_:\n\u001b[1;32m   1430\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_topic_vectors()\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/bertopic/_bertopic.py:3922\u001b[0m, in \u001b[0;36mBERTopic._extract_words_per_topic\u001b[0;34m(self, words, documents, c_tf_idf, calculate_aspects)\u001b[0m\n\u001b[1;32m   3920\u001b[0m         topics \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mextract_topics(\u001b[38;5;28mself\u001b[39m, documents, c_tf_idf, topics)\n\u001b[1;32m   3921\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_model, BaseRepresentation):\n\u001b[0;32m-> 3922\u001b[0m     topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepresentation_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextract_topics\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_tf_idf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtopics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3923\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_model, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   3924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepresentation_model\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMain\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/bertopic/representation/_textgeneration.py:152\u001b[0m, in \u001b[0;36mTextGeneration.extract_topics\u001b[0;34m(self, topic_model, documents, c_tf_idf, topics)\u001b[0m\n\u001b[1;32m    149\u001b[0m prompt = self._create_prompt(truncated_docs, topic, topics)\n\u001b[1;32m    150\u001b[0m self.prompts_.append(prompt)\n\u001b[0;32m--> 152\u001b[0m # Extract result from generator and use that as label\n\u001b[1;32m    153\u001b[0m topic_description = self.model(prompt, **self.pipeline_kwargs)\n\u001b[1;32m    154\u001b[0m topic_description = [(description[\"generated_text\"].replace(prompt, \"\"), 1) for description in topic_description]\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py:167\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    139\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m    Generate the output text(s) using text(s) given as inputs.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;124;03m          ids of the generated text.\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 167\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    169\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(args[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mlist\u001b[39m)\n\u001b[1;32m    170\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(el, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m el \u001b[38;5;129;01min\u001b[39;00m args[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    171\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mall\u001b[39m(\u001b[38;5;28mlen\u001b[39m(res) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result)\n\u001b[1;32m    172\u001b[0m     ):\n\u001b[1;32m    173\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m [res[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m result]\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1140\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\n\u001b[1;32m   1133\u001b[0m         \u001b[38;5;28miter\u001b[39m(\n\u001b[1;32m   1134\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_iterator(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         )\n\u001b[1;32m   1138\u001b[0m     )\n\u001b[1;32m   1139\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1140\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_single\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1147\u001b[0m, in \u001b[0;36mPipeline.run_single\u001b[0;34m(self, inputs, preprocess_params, forward_params, postprocess_params)\u001b[0m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_single\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs, preprocess_params, forward_params, postprocess_params):\n\u001b[1;32m   1146\u001b[0m     model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess(inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpreprocess_params)\n\u001b[0;32m-> 1147\u001b[0m     model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1148\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostprocess(model_outputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpostprocess_params)\n\u001b[1;32m   1149\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/pipelines/base.py:1046\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1044\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m inference_context():\n\u001b[1;32m   1045\u001b[0m         model_inputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m-> 1046\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mforward_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1047\u001b[0m         model_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/pipelines/text2text_generation.py:191\u001b[0m, in \u001b[0;36mText2TextGenerationPipeline._forward\u001b[0;34m(self, model_inputs, **generate_kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     in_b, input_length \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mshape(model_inputs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_inputs(\n\u001b[1;32m    187\u001b[0m     input_length,\n\u001b[1;32m    188\u001b[0m     generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmin_length),\n\u001b[1;32m    189\u001b[0m     generate_kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_length\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mmax_length),\n\u001b[1;32m    190\u001b[0m )\n\u001b[0;32m--> 191\u001b[0m output_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgenerate_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    192\u001b[0m out_b \u001b[38;5;241m=\u001b[39m output_ids\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:1548\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m   1541\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA decoder-only architecture is being used, but right-padding was detected! For correct \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1542\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgeneration results, please set `padding_side=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m` when initializing the tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1543\u001b[0m         )\n\u001b[1;32m   1545\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_kwargs:\n\u001b[1;32m   1546\u001b[0m     \u001b[38;5;66;03m# if model is encoder decoder encoder_outputs are created\u001b[39;00m\n\u001b[1;32m   1547\u001b[0m     \u001b[38;5;66;03m# and added to `model_kwargs`\u001b[39;00m\n\u001b[0;32m-> 1548\u001b[0m     model_kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_encoder_decoder_kwargs_for_generation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1549\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_input_name\u001b[49m\n\u001b[1;32m   1550\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;66;03m# 5. Prepare `input_ids` which will be used for auto-regressive generation\u001b[39;00m\n\u001b[1;32m   1553\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mis_encoder_decoder:\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/generation/utils.py:661\u001b[0m, in \u001b[0;36mGenerationMixin._prepare_encoder_decoder_kwargs_for_generation\u001b[0;34m(self, inputs_tensor, model_kwargs, model_input_name)\u001b[0m\n\u001b[1;32m    659\u001b[0m encoder_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreturn_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    660\u001b[0m encoder_kwargs[model_input_name] \u001b[38;5;241m=\u001b[39m inputs_tensor\n\u001b[0;32m--> 661\u001b[0m model_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoder_outputs\u001b[39m\u001b[38;5;124m\"\u001b[39m]: ModelOutput \u001b[38;5;241m=\u001b[39m \u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mencoder_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    663\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m model_kwargs\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:1113\u001b[0m, in \u001b[0;36mT5Stack.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, inputs_embeds, head_mask, cross_attn_head_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m   1099\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39mforward,\n\u001b[1;32m   1100\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1110\u001b[0m         output_attentions,\n\u001b[1;32m   1111\u001b[0m     )\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1113\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_decoder_position_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1121\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcross_attn_layer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1122\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1123\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1124\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1125\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# layer_outputs is a tuple with:\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;66;03m# hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\u001b[39;00m\n\u001b[1;32m   1129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:694\u001b[0m, in \u001b[0;36mT5Block.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, encoder_hidden_states, encoder_attention_mask, encoder_decoder_position_bias, layer_head_mask, cross_attn_layer_head_mask, past_key_value, use_cache, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m    691\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    692\u001b[0m     self_attn_past_key_value, cross_attn_past_key_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 694\u001b[0m self_attention_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mself_attn_past_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    703\u001b[0m hidden_states, present_key_value_state \u001b[38;5;241m=\u001b[39m self_attention_outputs[:\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    704\u001b[0m attention_outputs \u001b[38;5;241m=\u001b[39m self_attention_outputs[\u001b[38;5;241m2\u001b[39m:]  \u001b[38;5;66;03m# Keep self-attention outputs and relative position weights\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:600\u001b[0m, in \u001b[0;36mT5LayerSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_bias, layer_head_mask, past_key_value, use_cache, output_attentions)\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\n\u001b[1;32m    591\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    592\u001b[0m     hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    598\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    599\u001b[0m ):\n\u001b[0;32m--> 600\u001b[0m     normed_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    601\u001b[0m     attention_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mSelfAttention(\n\u001b[1;32m    602\u001b[0m         normed_hidden_states,\n\u001b[1;32m    603\u001b[0m         mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m         output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(attention_output[\u001b[38;5;241m0\u001b[39m])\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Workspace/learn_llm/.venv/lib/python3.11/site-packages/transformers/models/t5/modeling_t5.py:253\u001b[0m, in \u001b[0;36mT5LayerNorm.forward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states):\n\u001b[1;32m    248\u001b[0m     \u001b[38;5;66;03m# T5 uses a layer_norm which only scales and doesn't shift, which is also known as Root Mean\u001b[39;00m\n\u001b[1;32m    249\u001b[0m     \u001b[38;5;66;03m# Square Layer Normalization https://arxiv.org/abs/1910.07467 thus varience is calculated\u001b[39;00m\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;66;03m# w/o mean and there is no bias. Additionally we want to make sure that the accumulation for\u001b[39;00m\n\u001b[1;32m    251\u001b[0m     \u001b[38;5;66;03m# half-precision inputs is done in fp32\u001b[39;00m\n\u001b[0;32m--> 253\u001b[0m     variance \u001b[38;5;241m=\u001b[39m \u001b[43mhidden_states\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpow\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    254\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m hidden_states \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39mrsqrt(variance \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvariance_epsilon)\n\u001b[1;32m    256\u001b[0m     \u001b[38;5;66;03m# convert into half-precision if necessary\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from bertopic.representation import TextGeneration\n",
    "\n",
    "text_generator = pipeline(\"text2text-generation\", model=\"google/flan-t5-base\")\n",
    "representation_model = TextGeneration(text_generator, prompt=prompt)\n",
    "\n",
    "topic_model.update_topics(docs=abstracts, representation_model=representation_model)\n",
    "\n",
    "topic_differences(topic_model, original_topics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: speech | asr | recognition | acoustic | endtoend             -> ASR and TTS for reinforcement learning |  |  |  | \n",
      "Topic 1: hate | offensive | detection | speech | toxic                -> Automatic Hate Speech Detection on Social Media |  |  |  | \n",
      "Topic 2: summarization | summaries | summary | abstractive | extractive -> Summarization without ground-truth summaries |  |  |  | \n",
      "Topic 3: prompt | fewshot | prompts | incontext | tuning              -> CP-Tuning |  |  |  | \n",
      "Topic 4: ner | named | entity | nested | recognition                  -> Named Entity Recognition |  |  |  | \n",
      "Topic 5: clinical | medical | notes | patient | patients              -> Attention-Based Deep Learning for Clinical Progress Notes |  |  |  | \n",
      "Topic 6: word | embeddings | embedding | vectors | similarity         -> Word embeddings |  |  |  | \n",
      "Topic 7: bias | gender | biases | debiasing | fairness                -> Gender bias in natural language processing |  |  |  | \n",
      "Topic 8: parsing | dependency | parser | parsers | transitionbased    -> Joint Learning of Syntactic and Syntactic Parsing |  |  |  | \n",
      "Topic 9: explanations | explanation | rationales | rationale | interpretability -> Natural-language Inference over Label-specific Explanations |  |  |  | \n",
      "Topic 10: mt | qe | translation | postediting | ape                    -> Quality Estimation of Machine Translation |  |  |  | \n",
      "Topic 11: topic | topics | lda | latent | documents                    -> Topic Modeling and Coherence |  |  |  | \n",
      "Topic 12: legal | court | law | case | documents                       -> Legal NLP |  |  |  | \n",
      "Topic 13: crosslingual | multilingual | transfer | languages | mbert   -> Multilingual BERT |  |  |  | \n",
      "Topic 14: commonsense | reasoning | knowledge | commonsenseqa | conceptnet -> Commonsense Learning with Knowledge-in-the-Loop Question Answering |  |  |  | \n",
      "Topic 15: adversarial | attacks | attack | robustness | examples       -> Textual Adversarial Attacks |  |  |  | \n",
      "Topic 16: graph | knowledge | link | kg | graphs                       -> Knowledge Graph Embedding |  |  |  | \n",
      "Topic 17: argument | arguments | argumentation | argumentative | mining -> Argument mining in online debate |  |  |  | \n",
      "Topic 18: aspect | absa | sentiment | aspectbased | opinion            -> Aspect-based sentiment analysis |  |  |  | \n",
      "Topic 19: comprehension | mrc | reading | answer | passage             -> Reading comprehension |  |  |  | \n",
      "Topic 20: linking | entity | el | entities | mentions                  -> NASTyLinker: A supervised approach to entity linking |  |  |  | \n",
      "Topic 21: fake | news | detection | articles | media                   -> Fake News Detection |  |  |  | \n",
      "Topic 22: pruning | quantization | compression | bert | transformer    -> Optimized BERT Surgeon for Unstructured Weight Pruning |  |  |  | \n",
      "Topic 23: dst | tracking | state | dialogue | slot                     -> Dialogue State Tracking (DST) |  |  |  | \n",
      "Topic 24: emotion | emotions | affective | intensity | anger           -> Emotion analysis in tweets |  |  |  | \n",
      "Topic 25: style | transfer | content | formality | styles              -> Text Style Transfer |  |  |  | \n",
      "Topic 26: paraphrase | paraphrases | plagiarism | paraphrasing | generation -> Paraphrase generation: a pipelined system for paraphrase generation |  |  |  | \n",
      "Topic 27: law | zipfs | frequency | entropy | words                    -> Zipf's law for word frequency |  |  |  | \n",
      "Topic 28: political | news | media | speeches | bias                   -> Political stance prediction for news articles |  |  |  | \n",
      "Topic 29: transformer | attention | selfattention | position | transformers -> Transformers: A Novel Architecture for Optimized Transformers |  |  |  | \n",
      "Topic 30: bilingual | crosslingual | embeddings | bli | spaces         -> Unsupervised machine translation |  |  |  | \n",
      "Topic 31: generation | text | decoding | sampling | beam               -> Decoding and decoding of neural text |  |  |  | \n",
      "Topic 32: discourse | rst | parsing | implicit | relations             -> Parser for Persian Language |  |  |  | \n",
      "Topic 33: recurrent | rnn | lstm | rnns | memory                       -> Recurrent neural networks |  |  |  | \n",
      "Topic 34: claim | factchecking | evidence | claims | verification      -> Hierarchical Evidence Set Modeling for Automated Fact Checking |  |  |  | \n",
      "Topic 35: depression | mental | health | posts | media                 -> Deep Knowledge-aware Depression Detection |  |  |  | \n",
      "Topic 36: event | extraction | argument | events | arguments           -> Event Extraction |  |  |  | \n",
      "Topic 37: classification | convolutional | text | cnn | networks       -> Text classification using convolutional neural network |  |  |  | \n",
      "Topic 38: sql | texttosql | queries | query | database                 -> Text-to-SQL parsing |  |  |  | \n",
      "Topic 39: gec | correction | grammatical | error | errors              -> Grammatical Error Correction |  |  |  | \n",
      "Topic 40: intent | intents | ood | utterances | detection              -> Deep Aligned Clustering for Intent Discovery |  |  |  | \n",
      "Topic 41: distillation | teacher | student | kd | knowledge            -> Knowledge Distillation |  |  |  | \n",
      "Topic 42: coreference | resolution | mentions | mention | ontonotes    -> A hybrid rule-neural coreference resolution system for mention detection |  |  |  | \n",
      "Topic 43: financial | stock | market | price | sentiment               -> Financial sentiment analysis |  |  |  | \n",
      "Topic 44: code | programming | program | commit | software             -> Contextualized code representation learning for code commit message generation |  |  |  | \n",
      "Topic 45: empathetic | empathy | emotion | emotional | response        -> Empathetic Dialogues |  |  |  | \n",
      "Topic 46: sarcasm | sarcastic | irony | detection | tweets             -> sarcasm detection in text |  |  |  | \n",
      "Topic 47: sentiment | analysis | deep | reviews | classification       -> Sentiment Analysis in Persian Language |  |  |  | \n",
      "Topic 48: wsd | sense | disambiguation | senses | word                 -> Word Sense Disambiguation |  |  |  | \n",
      "Topic 49: policy | dialogue | dialog | reinforcement | rl              -> Dialogue Policy Learning with Reinforcement Learning |  |  |  | \n",
      "Topic 50: annotation | tools | xml | formats | olac                    -> OLAC: a new digital infrastructure for language resource discovery |  |  |  | \n",
      "Topic 51: translation | zeroshot | multilingual | nmt | mnmt           -> Multilingual Neural Machine Translation |  |  |  | \n",
      "Topic 52: slang | geographic | location | geolocation | twitter        -> Slang |  |  |  | \n",
      "Topic 53: summarization | meeting | dialogue | summaries | meetings    -> Abstractive Summarization of Meetings |  |  |  | \n",
      "Topic 54: segmentation | chinese | cws | word | tagging                -> Chinese word segmentation |  |  |  | \n",
      "Topic 55: multimodal | visual | dialog | video | avsd                  -> Audio Visual Scene-Aware Dialog |  |  |  | \n",
      "Topic 56: nat | nonautoregressive | autoregressive | nar | translation -> Non-Autoregressive Neural Machine Translation |  |  |  | \n",
      "Topic 57: instructions | agent | navigation | environment | environments -> A neural agent for navigation |  |  |  | \n",
      "Topic 58: tweets | twitter | hashtag | hashtags | tweet                -> Hashtag-guided Tweet Classification Model for Twitter |  |  |  | \n",
      "Topic 59: parsing | semantic | parser | logical | parsers              -> semantic parsing |  |  |  | \n",
      "Topic 60: change | diachronic | semantic | time | shifts               -> semantic change |  |  |  | \n",
      "Topic 61: srl | role | labeling | semantic | argument                  -> Semantic role labeling for Vietnamese |  |  |  | \n",
      "Topic 62: image | captioning | captions | caption | images             -> Captioning from Images |  |  |  | \n",
      "Topic 63: relation | distant | supervision | extraction | noisy        -> Distant Supervision for Relation Extraction |  |  |  | \n",
      "Topic 64: questions | question | qg | generation | answer              -> Automatic question generation from text |  |  |  | \n",
      "Topic 65: math | mathematical | mwp | mwps | problems                  -> A neural approach to automatically solve math word problems by operating symbols according to their semantic meanings in |  |  |  | \n",
      "Topic 66: responses | response | generation | dialogue | diversity     -> A new automatic evaluation metric for response diversity |  |  |  | \n",
      "Topic 67: poetry | poems | lyrics | poem | chinese                     -> Poetry Generation |  |  |  | \n",
      "Topic 68: domain | adaptation | domains | target | source              -> Domain adaptation for Natural Language Processing |  |  |  | \n",
      "Topic 69: agreement | syntactic | grammatical | rnns | lstms           -> Gender agreement in French |  |  |  | \n",
      "Topic 70: simplification | ts | sentence | simplified | lexical        -> Text Simplifying |  |  |  | \n",
      "Topic 71: augmentation | data | augmented | mixup | classification     -> Data Augmentation for Few-Shot Text Classification |  |  |  | \n",
      "Topic 72: vqa | visual | question | image | answering                  -> eXplainable Question Answering |  |  |  | \n",
      "Topic 73: sentiment | polarity | reviews | analysis | negative         -> Sentiment Analysis |  |  |  | \n",
      "Topic 74: gender | bias | translation | coreference | gendered         -> Gender bias in coreference resolution systems |  |  |  | \n",
      "Topic 75: stance | detection | stances | targets | target              -> stance detection in tweets |  |  |  | \n",
      "Topic 76: amr | smatch | meaning | parser | parsing                    -> AMR-to-Text |  |  |  | \n",
      "Topic 77: multimodal | mmt | visual | image | translation              -> Multimodal machine translation with visual context |  |  |  | \n",
      "Topic 78: temporal | events | event | relations | timeml               -> Temporal Relation Extraction from Text |  |  |  | \n",
      "Topic 79: knowledge | knowledgegrounded | dialogue | responses | response -> Knowledge-grounded dialogue |  |  |  | \n",
      "Topic 80: qa | question | answer | answering | questions               -> Extractive QA: A Model for Extractive QA |  |  |  | \n",
      "Topic 81: citation | papers | citations | scientific | academic        -> Using a citation graph to generate multiple citations |  |  |  | \n",
      "Topic 82: forgetting | continual | catastrophic | lifelong | cl        -> Continuous Prompt Tuning for Continuous Language Learning |  |  |  | \n",
      "Topic 83: privacy | private | differential | policies | privacypreserving -> Privacy-preserving NLP |  |  |  | \n",
      "Topic 84: nlg | metrics | evaluation | human | metric                  -> Automatic Evaluation Metrics for Natural Language Generation |  |  |  | \n",
      "Topic 85: layout | document | documents | extraction | information     -> ERNIE-Layout |  |  |  | \n",
      "Topic 86: pos | tagger | tagging | taggers | partofspeech              -> POS tagging |  |  |  | \n",
      "Topic 87: story | stories | plot | generation | narrative              -> Automatic Story Generation |  |  |  | \n",
      "Topic 88: biomedical | medical | concept | concepts | umls             -> Biomedical Entity Linking |  |  |  | \n",
      "Topic 89: translation | smt | indian | machine | statistical           -> Statistical Machine Translation (SMT) and Rule-Based Machine Translation (RBMT) for |  |  |  | \n",
      "Topic 90: emotion | erc | emotions | conversation | emotional          -> Emotion Recognition in Conversation |  |  |  | \n",
      "Topic 91: similarity | sentence | sts | semantic | embeddings          -> Sentence-BERT: A Modification of BERT and RoBERTa |  |  |  | \n",
      "Topic 92: gans | gan | adversarial | generative | discriminator        -> Generative Adversarial Networks for Text Generation |  |  |  | \n",
      "Topic 93: humor | pun | puns | humorous | jokes                        -> Humor detection using text generation |  |  |  | \n",
      "Topic 94: nli | negation | inference | entailment | premise            -> Natural Language Inference |  |  |  | \n",
      "Topic 95: compositional | generalization | compositionality | scan | seq2seq -> Compositional generalization in math word problems |  |  |  | \n",
      "Topic 96: morphological | inflection | lemma | lemmatization | inflected -> Morphological inflection |  |  |  | \n",
      "Topic 97: label | labels | multilabel | classification | hierarchy     -> Multi-label text classification |  |  |  | \n",
      "Topic 98: contrastive | sentence | learning | sts | unsupervised       -> Contrastive Learning for Sentence Embedding |  |  |  | \n",
      "Topic 99: persona | personalized | personas | dialogue | responses     -> Personalized dialogue using persona-sparse dialogue data |  |  |  | \n",
      "Topic 100: slot | intent | filling | slu | joint                        -> Attention-Based Neural Network Model for Joint Intent Classification and Slot Filling |  |  |  | \n",
      "Topic 101: arabic | transliteration | script | arabizi | morphological  -> Arabic Language |  |  |  | \n",
      "Topic 102: crisis | disaster | disasters | emergency | tweets           -> Human-annotated Twitter Corpus |  |  |  | \n",
      "Topic 103: probing | probe | linguistic | probes | bert                 -> Probing Language Models |  |  |  | \n",
      "Topic 104: active | al | learning | annotation | uncertainty            -> Active learning for natural language processing |  |  |  | \n",
      "Topic 105: relation | re | extraction | docre | entity                  -> CORE: A Framework for Relation Extraction |  |  |  | \n",
      "Topic 106: authorship | attribution | author | authors | av             -> Authorship attribution in medieval texts |  |  |  | \n",
      "Topic 107: sign | asl | deaf | signed | gloss                           -> Sign Language Translation |  |  |  | \n",
      "Topic 108: codeswitching | cs | codeswitched | multilingual | monolingual -> Code-switching language models: a novel approach |  |  |  | \n",
      "Topic 109: latent | variational | vae | variables | posterior           -> Variational Autoencoders |  |  |  | \n",
      "Topic 110: agents | communication | emergent | referential | games      -> Emergent Languages |  |  |  | \n",
      "Topic 111: crossings | dependency | order | minimization | syntactic    -> Dependency distance minimization |  |  |  | \n",
      "Topic 112: metaphor | metaphors | metaphorical | similes | figurative   -> Metaphoric Polysemy Detection |  |  |  | \n",
      "Topic 113: codemixed | sentiment | codemixing | hindienglish | analysis -> Sentiment Analysis in Code-Mixed Tweets |  |  |  | \n",
      "Topic 114: bioasq | questions | question | qa | biomedical              -> Biomedical Question Answering |  |  |  | \n",
      "Topic 115: retrieval | dense | retrievers | passage | retriever         -> dense phrase retrieval |  |  |  | \n",
      "Topic 116: kbqa | question | answering | base | kgqa                    -> Complex Knowledge Base Question Answering |  |  |  | \n",
      "Topic 117: revision | writing | editing | revisions | edit              -> Text Revision |  |  |  | \n",
      "Topic 118: emotion | recognition | speech | ser | iemocap               -> Speech Emotion Recognition |  |  |  | \n",
      "Topic 119: games | game | agents | textbased | agent                    -> Text-based computer games: exploration and imitation-learning-based agents |  |  |  | \n",
      "Topic 120: product | ecommerce | attribute | products | attributes      -> A System for Detecting Identifiable Identifiable Identif |  |  |  | \n",
      "Topic 121: domain | adaptation | nmt | indomain | translation           -> Domain adaptation for neural machine translation |  |  |  | \n",
      "Topic 122: emoji | emojis | emoticons | sentiment | twitter             -> Emojis, Emojis, Emojis, Emojis, Emojis |  |  |  | \n",
      "Topic 123: keyphrase | keyphrases | absent | generation | document      -> Keyphrase generation |  |  |  | \n",
      "Topic 124: causal | causality | event | causeeffect | confounders       -> causal reasoning |  |  |  | \n",
      "Topic 125: grammars | grammar | contextfree | pcfgs | parsing           -> Parsing for Minimalist Grammars |  |  |  | \n",
      "Topic 126: evaluation | metrics | dialogue | dialog | opendomain        -> automatic evaluation of dialogue response generation |  |  |  | \n",
      "Topic 127: finetuning | parameters | parameterefficient | tuning | peft -> Parameter-Efficient Fine-Tuning |  |  |  | \n",
      "Topic 128: multihop | reasoning | qa | singlehop | subquestions         -> Multi-hop question answering |  |  |  | \n",
      "Topic 129: personality | traits | mbti | app | big                      -> Personality Detection by Computational Models |  |  |  | \n",
      "Topic 130: nmt | subword | bpe | segmentation | translation             -> Subword segmentation in machine translation |  |  |  | \n",
      "Topic 131: covid19 | pandemic | tweets | health | public                -> Covid-19 vaccine: a social media analysis |  |  |  | \n",
      "Topic 132: alignment | kgs | ea | entity | entities                     -> Active Learning for Entity Alignment |  |  |  | \n",
      "Topic 133: prize | alexa | conversational | competition | chatbot       -> Alexa Prize: A Framework for Personalized Conversational Agents |  |  |  | \n",
      "Topic 134: typing | entity | type | types | finegrained                 -> Entity Typing |  |  |  | \n",
      "Topic 135: act | da | dialog | dialogue | utterances                    -> Dialogue Act Segmentation for Vietnamese |  |  |  | \n",
      "Topic 136: cls | summarization | crosslingual | summaries | abstractive -> Neural Cross-Lingual Summarization |  |  |  | \n",
      "Topic 137: tagging | chunking | sequence | labeling | crf               -> A novel neural sequence labeling architecture |  |  |  | \n",
      "Topic 138: masking | mlm | masked | pretraining | tokens                -> masking, masked language model, masked, pre-training, m |  |  |  | \n",
      "Topic 139: graph | nodes | node | gnn | gcn                             -> graph neural networks |  |  |  | \n",
      "Topic 140: arabic | sentiment | analysis | dialects | msa               -> Arabic Sentiment Analysis |  |  |  | \n",
      "Topic 141: typological | tokenization | subword | tokenizer | tokenizers -> A novel tokenizer for linguistic typology |  |  |  | \n",
      "Topic 142: qa | answering | question | multilingual | languages         -> Multilingual Question Answering |  |  |  | \n",
      "Topic 143: reviews | summaries | summarization | opinion | review       -> Summarization from customer reviews |  |  |  | \n",
      "Topic 144: answer | question | qa | answers | answering                 -> Deep Learning for Question Answering |  |  |  | \n",
      "Topic 145: annotators | crowdsourcing | disagreement | annotation | annotator -> Learning Ambiguity from Crowd Sequential Annotations |  |  |  | \n",
      "Topic 146: radiology | reports | chest | imaging | medical              -> A novel method for automatic generation of semi-structured representations of radiology reports |  |  |  | \n",
      "Topic 147: moral | ethical | morality | ethics | foundations            -> Text-based Moral Reasoning |  |  |  | \n",
      "Topic 148: table | tabletotext | tables | generation | datatotext       -> Table-to-text generation with pre-trained language model |  |  |  | \n",
      "Topic 149: documentlevel | context | nmt | contextaware | translation   -> Hierarchical Attention for Neural Machine Translation |  |  |  | \n",
      "Topic 150: selfattention | attention | transformer | multihead | sans   -> A new way of learning dependencies through a context in multi-head using convolution |  |  |  | \n",
      "Topic 151: transfer | multitask | finetuning | tasks | tl               -> Transfer Learning |  |  |  | \n",
      "Topic 152: quantum | discocat | entanglement | meaning | meanings       -> Quantum Natural Language Processing |  |  |  | \n",
      "Topic 153: codeswitching | cs | asr | speech | monolingual              -> Code-Switching Automatic Speech Recognition System with Data-Adjusted Language Model |  |  |  | \n",
      "Topic 154: nmt | translation | attention | source | attentionbased      -> Attention-based Neural Machine Translation |  |  |  | \n",
      "Topic 155: phylogenetic | cognate | cognates | indoeuropean | family    -> Phylogenetic inference |  |  |  | \n",
      "Topic 156: ad | dementia | alzheimers | disease | cognitive             -> Automatic detection of Alzheimer's disease using speech and lexical features |  |  |  | \n",
      "Topic 157: reasoning | cot | prompting | llms | chainofthought          -> CoT prompting |  |  |  | \n",
      "Topic 158: multimodal | fusion | sentiment | modalities | modality      -> Multimodal Sentiment Analysis |  |  |  | \n",
      "Topic 159: propaganda | subtask | tc | si | techniques                  -> Detection of Propaganda Techniques in News Articles |  |  |  | \n",
      "Topic 160: localisation | keyword | captions | spoken | images          -> Keyword Localisation with Visually Grounded Speech Models |  |  |  | \n",
      "Topic 161: gaze | eyetracking | eye | reading | cognitive               -> Eye-tracking in natural language processing |  |  |  | \n",
      "Topic 162: dialect | identification | arabic | dialects | nadi          -> Arabic Dialect Identification: A Critical Survey |  |  |  | \n",
      "Topic 163: ocr | optical | character | errors | scanned                 -> Optical Character Recognition for Early Books |  |  |  | \n",
      "Topic 164: relation | biomedical | extraction | biological | relations  -> Relation Extraction in Biomedical Text Mining |  |  |  | \n",
      "Topic 165: medical | doctors | diagnosis | patients | symptoms          -> Dialogue Modeling for Diagnosis and Diagnosis in the Medical Domain |  |  |  | \n",
      "Topic 166: rumor | rumors | rumour | rumours | stance                   -> rumor detection on Twitter |  |  |  | \n",
      "Topic 167: robustness | nmt | noise | translation | clean               -> Improved Neural Machine Translation Robustness to Noise |  |  |  | \n",
      "Topic 168: event | events | script | narrative | scripts                -> Script Event Prediction |  |  |  | \n",
      "Topic 169: readability | assessment | difficulty | reading | features   -> Text Readability Assessment |  |  |  | \n",
      "Topic 170: biased | biases | bias | debiasing | nli                     -> Debiasing contrastive learning for generalization of deep neural networks |  |  |  | \n",
      "Topic 171: backdoor | attacks | attack | triggers | poisoned            -> Backdoor Attack |  |  |  | \n",
      "Topic 172: essay | aes | scoring | essays | automated                   -> Automated Essay Scoring: AES |  |  |  | \n",
      "Topic 173: nlp | science | research | processing | generalisation       -> nlp, ml, rf, ml, re |  |  |  | \n",
      "Topic 174: translation | recurrent | decoder | nmt | mongolian          -> A new neural network architecture for machine translation |  |  |  | \n",
      "Topic 175: dialogue | response | utterance | utterances | context       -> Knowledge Distillation for Dialogue Sequence Labeling |  |  |  | \n",
      "Topic 176: scientific | scholarly | papers | publications | research    -> Natural Language Processing and Machine Learning for Knowledge Graphs |  |  |  | \n",
      "Topic 177: counseling | psychotherapy | therapy | mental | health       -> Cognitive behavioral therapy |  |  |  | \n",
      "Topic 178: idioms | idiomatic | idiom | expressions | literal           -> idiom translation |  |  |  | \n",
      "Topic 179: covid19 | tweets | informative | wnut2020 | health           -> Covid-19: Identification of Informative Tweets |  |  |  | \n",
      "Topic 180: openie | oie | open | extractions | tuples                   -> Open Information Extraction |  |  |  | \n",
      "Topic 181: patent | patents | claim | landscaping | claims              -> Patent Analysis based on feature vector space model |  |  |  | \n",
      "Topic 182: recommendation | crs | recommender | items | item            -> Conversational Recommendation Systems |  |  |  | \n",
      "Topic 183: surprisal | reading | n400 | brain | erps                    -> Predicting reading time by entropy and surprisal |  |  |  | \n",
      "Topic 184: reasoning | proof | proofs | logical | logic                 -> ProofWriter: A generative model for generating proofs |  |  |  | \n",
      "Topic 185: submission | translation | submissions | news | wmt          -> translation, wmt, languages, languages, languages, languages, languages, languages, |  |  |  | \n",
      "Topic 186: matching | short | phrase | sentence | attention             -> Text Matching with Deep Info Max |  |  |  | \n",
      "Topic 187: protest | event | events | political | sociopolitical        -> Multilingual Protest Event Detection using ClassBases |  |  |  | \n",
      "Topic 188: acronym | acronyms | abbreviations | abbreviation | disambiguation -> Acronym Disambiguation |  |  |  | \n",
      "Topic 189: relation | fewshot | relations | prerequisite | fewrel       -> zslrc, fewshot, relation, concept, zsbert, |  |  |  | \n",
      "Topic 190: vietnamese | ner | chinese | vlsp | named                    -> Named Entity Recognition in Vietnamese |  |  |  | \n",
      "Topic 191: backtranslation | nmt | monolingual | translation | parallel -> Back-Translation for Neural Machine Translation |  |  |  | \n",
      "Topic 192: treebanks | treebank | dependency | parsing | parser         -> Multilingual Dependency Parsing |  |  |  | \n",
      "Topic 193: taxonomy | taxonomies | concepts | concept | expansion       -> Octet for Online Catalog Taxonomy EnrichmenT |  |  |  | \n",
      "Topic 194: cybersecurity | security | cyber | forums | vulnerability    -> SecureBERT: A Neural Network Based System for Detecting Cyber Threat Intelligence |  |  |  | \n",
      "Topic 195: frame | framenet | frames | parsing | semantic               -> Semantic Frame parsing |  |  |  | \n",
      "Topic 196: storytelling | story | visual | images | stories             -> Visual storytelling: a novel approach to storytelling |  |  |  | \n",
      "Topic 197: multimodal | visual | singlemodal | unimodal | modalities    -> Multimodal Machine Learning |  |  |  | \n",
      "Topic 198: coherence | discourse | phsic | paragraph | discoscore       -> Coherence in Text |  |  |  | \n",
      "Topic 199: bert | multilingual | monolingual | lao | estonian           -> Multilingual BERT model for Estonian |  |  |  | \n",
      "Topic 200: chatbot | chatgpt | chatbots | chatgpts | service            -> Chatbots and ChatGPT |  |  |  | \n",
      "Topic 201: ad | search | query | queries | ads                          -> VisualTextRank: An Unsupervised Method for Generate Related Ads |  |  |  | \n",
      "Topic 202: medical | summaries | summarization | clinical | physicians  -> Summarization of medical conversations |  |  |  | \n",
      "Topic 203: rl | reward | feedback | reinforcement | preferences         -> Reinforcement Learning for Text Generation |  |  |  | \n",
      "Topic 204: anaphora | pronouns | resolution | referents | noun          -> Anaphora Resolution in Japanese Dialogues |  |  |  | \n",
      "Topic 205: lstm | rnns | cfls | stack | lstms                           -> Nondeterministic Stack RNN |  |  |  | \n",
      "Topic 206: hypernymy | distributional | term | cohyponymy | hypernyms   -> Hypernyms |  |  |  | \n",
      "Topic 207: recipes | recipe | cooking | food | ingredients              -> Recipe Generation |  |  |  | \n",
      "Topic 208: nmt | toolkit | sgnmt | translation | sockeye                -> Joey NMT |  |  |  | \n",
      "Topic 209: arabic | arabert | arlue | argen | dialectal                 -> Arabic document image-based classifier |  |  |  | \n",
      "Topic 210: diacritization | diacritics | restoration | diacritic | arabic -> Diacritic restoration in Arabic Automatic Speech Recognition |  |  |  | \n",
      "Topic 211: beam | search | mbr | decoding | hypotheses                  -> Decoding Bayes-risk for Machine Translation |  |  |  | \n",
      "Topic 212: keyphrase | extraction | keyword | keywords | keyphrases     -> Keyphrase extraction and keyphrase assignment |  |  |  | \n",
      "Topic 213: story | narrative | stories | ending | narratives            -> Automated story generation |  |  |  | \n",
      "Topic 214: tables | table | tabular | reasoning | statement             -> Table-based reasoning |  |  |  | \n",
      "Topic 215: complexity | cwi | lexical | texts | simplification          -> Lexical Complexity Prediction |  |  |  | \n",
      "Topic 216: parallel | translation | machine | pairs | cherokee          -> Parallel corpora for Korean |  |  |  | \n",
      "Topic 217: coreference | event | cdcr | crossdocument | resolution      -> Event Coreference Resolution |  |  |  | \n",
      "Topic 218: opendomain | passages | qa | reader | retrieval              -> Re-ranking for Question Answering |  |  |  | \n",
      "Topic 219: leaderboards | significance | leaderboard | dynabench | nlp  -> NLPStatTest: A toolkit for comparing NLP system performance |  |  |  | \n",
      "Topic 220: multitask | mtl | tasks | sharing | matinf                   -> Multi-Task Learning |  |  |  | \n",
      "Topic 221: dialog | dialogue | goaloriented | taskoriented | odd        -> SPACE-3: A Novel Multi-Turn Dialog Model Learning from Large-Scal |  |  |  | \n",
      "Topic 222: video | videos | nlvl | moment | vslnet                      -> Video localization |  |  |  | \n",
      "Topic 223: questions | students | educational | difficulty | qg         -> Automatic Multiple Choice Question Generation for Math Word Problem Solving |  |  |  | \n",
      "Topic 224: entailment | rte | logicbased | comparatives | textual       -> XTE - Explainable Text Entailment |  |  |  | \n",
      "Topic 225: biomedical | biobert | pubmed | pretraining | bioformer      -> BioALBERT: Pre-trained Language Model for Biomedical Text Mining |  |  |  | \n",
      "Topic 226: ecpe | emotion | cause | emotioncause | ece                  -> Emotion-cause pair extraction |  |  |  | \n",
      "Topic 227: codemixed | codemixing | hinglish | hindienglish | hindi     -> Code-Mixed Text Data |  |  |  | \n",
      "Topic 228: gender | names | profiling | euphemisms | author             -> Gender Prediction in Code-Mixed Content |  |  |  | \n",
      "Topic 229: compounds | noun | compound | compositionality | distributional -> Noun compound splitting and idiomatic compound detection for German language |  |  |  | \n",
      "Topic 230: literary | authorship | glec | shakespeare | author          -> English literary text |  |  |  | \n",
      "Topic 231: spelling | correction | misspellings | misspelling | realword -> Spelling correction |  |  |  | \n",
      "Topic 232: reviews | fake | review | deceptive | spam                   -> Fake reviews: Detecting and Detecting Fake Reviews |  |  |  | \n",
      "Topic 233: deidentification | notes | medical | clinical | patient      -> De-identification of patient notes |  |  |  | \n",
      "Topic 234: stemming | stemmer | root | arabic | stem                    -> Arabic Text Classification and Stemming |  |  |  | \n",
      "Topic 235: brain | fmri | recordings | activity | meg                   -> Brain-based Interpretation of Language |  |  |  | \n",
      "Topic 236: headline | headlines | news | articles | attractive          -> A novel neural model for headline generation |  |  |  | \n",
      "Topic 237: treelstm | recursive | tree | structure | parse              -> Syntactic Structure and Syntactic Structure in Recursive Neural Networks |  |  |  | \n",
      "Topic 238: layers | transformer | encoder | translation | decoder       -> Transformer model |  |  |  | \n",
      "Topic 239: grading | students | student | asag | answers                -> Automatic short answer grading |  |  |  | \n",
      "Topic 240: calculus | lambek | logic | mathcal | lambekgrishin          -> Lambek calculus |  |  |  | \n",
      "Topic 241: discourse | translations | translation | contextaware | phenomena -> Docrepair: A novel approach to improve translation quality from discourse perspective |  |  |  | \n",
      "Topic 242: tweets | drug | mining | health | medication                 -> pharmacovigilance using Twitter |  |  |  | \n",
      "Topic 243: diffusion | continuous | generation | discrete | genie       -> Text-Diffusion Models for Natural Language Generation |  |  |  | \n",
      "Topic 244: bnlp | endangered | gondi | technologies | community         -> revitalize Cherokee |  |  |  | \n",
      "Topic 245: materials | synthesis | material | scientific | science      -> Extracting Structured Representations of Synthesis Procedures from Materials Science Literature |  |  |  | \n",
      "Topic 246: visual | grounded | grounding | visually | vgnsl             -> Visual Grounding of Word Embeddings |  |  |  | \n",
      "Topic 247: drug | adverse | ade | pharmacovigilance | reactions         -> Adverse drug events |  |  |  | \n",
      "Topic 248: reviews | helpfulness | review | customers | helpful         -> Argumentation in Review Texts Increases Review Helpfulness |  |  |  | \n",
      "Topic 249: numerical | numbers | numeracy | reasoning | mathematical    -> Numerical Question Answering |  |  |  | \n",
      "Topic 250: simultaneous | simt | latency | fullsentence | translation   -> Simultaneous machine translation |  |  |  | \n",
      "Topic 251: ontology | owl | ol | ontologies | web                       -> Natural Language Generation for OWL ontologies |  |  |  | \n",
      "Topic 252: triplets | aspect | aste | opinion | triplet                 -> Aspect Sentiment Triplet Extraction |  |  |  | \n",
      "Topic 253: vl | visual | multimodal | textonly | valm                   -> Multimodal Pretraining of Text-only Language Models |  |  |  | \n",
      "Topic 254: clustering | cluster | hmff | ek | kmeans                    -> Text Clustering |  |  |  | \n",
      "Topic 255: latent | responses | variational | diversity | variables     -> Variational Transformers for Dialogue Generation |  |  |  | \n",
      "Topic 256: tod | crosslingual | multilingual | dialogue | taskoriented  -> Multilingual Task-Oriented Dialogue |  |  |  | \n",
      "Topic 257: vaccines | vaccine | misinformation | covid19 | hesitancy    -> CoVaxLies: A Novel Method for Detecting Misinformation on Twitter |  |  |  | \n",
      "Topic 258: knowledge | kgs | kg | knowledgeenhanced | lms               -> Knowledge-enhanced language models |  |  |  | \n",
      "Topic 259: graphtotext | graph | webnlg | g2t | graphtext               -> Reinforcement Learning for Knowledge Graph Generation |  |  |  | \n",
      "Topic 260: ood | vl | ensembles | lmentry | rsg                         -> LMentry: A Benchmark for Large Language Models |  |  |  | \n",
      "Topic 261: color | colour | colors | ib | naming                        -> Color Language |  |  |  | \n",
      "Topic 262: categorical | compositional | distributional | semantics | frobenius -> Compositional distributional semantics |  |  |  | \n",
      "Topic 263: hebrew | rabbinic | alephbert | bible | morphological        -> Hebrew NLP |  |  |  | \n",
      "Topic 264: slu | dialog | turntaking | history | nlm                    -> A Hierarchical Conversation Model for Multi-Turn Dialogue Understanding |  |  |  | \n",
      "Topic 265: job | skill | skills | soft | resumes                        -> Detecting Soft Skills from Job Descriptions |  |  |  | \n",
      "Topic 266: morphological | analyzer | affixes | lemmatization | uzbek   -> Morphological Analysis |  |  |  | \n",
      "Topic 267: semantics | montague | logic | pregroup | donkey             -> semantics |  |  |  | \n",
      "Topic 268: matching | retrievalbased | multiturn | response | chatbots  -> Sequential matching for multi-turn response selection in retrieval-based chatbots |  |  |  | \n",
      "Topic 269: normalization | historical | spelling | ligurian | swiss     -> Text normalization |  |  |  | \n",
      "Topic 270: peer | review | reviewers | reviews | reviewing              -> Peer Review: A Multi-Task Learning Approach |  |  |  | \n",
      "Topic 271: rogets | thesaurus | openhownet | wordnet | lexical          -> OpenHowNet: A Novel Lexical Knowledge Base |  |  |  | \n",
      "Topic 272: alignment | parallel | alignments | word | pairs             -> Word Alignment using Language-Image Pretraining |  |  |  | \n",
      "Topic 273: csc | spelling | chinese | check | correction                -> Chinese spelling check |  |  |  | \n",
      "Topic 274: termhood | clir | comparability | bilingual | dictionary     -> Multi-level termhood for bilingual terminology extraction |  |  |  | \n",
      "Topic 275: clickbait | clickbaits | headlines | news | attract          -> Clickbait: A Computational Clickbait Detection Model |  |  |  | \n",
      "Topic 276: african | languages | afrolid | kencorpus | tools            -> African languages |  |  |  | \n",
      "Topic 277: mrc | comprehension | reading | vietnamese | chinese         -> Vietnamese machine reading comprehension |  |  |  | \n",
      "Topic 278: sanskrit | sandhi | euphonic | conjunctions | bhartrhari     -> Search for words in Sanskrit E-text |  |  |  | \n",
      "Topic 279: spam | email | antispam | filters | filter                   -> Anti-spam Filters |  |  |  | \n",
      "Topic 280: federated | fl | clients | privacy | client                  -> Federated Learning for Natural Language Processing |  |  |  | \n",
      "Topic 281: treebank | ud | annotation | treebanks | dependency          -> Treebanking and its Annotation in Turkish |  |  |  | \n",
      "Topic 282: analogical | analogies | analogy | lrme | mwp                -> Analogical Reasoning |  |  |  | \n",
      "Topic 283: subtask | subtasks | abstract | semeval2021 | recam          -> SemEval-2021 Task4: Reading Comprehension of Abstract Meaning (ReCAM |  |  |  | \n",
      "Topic 284: chinese | tang | historical | china | mainland               -> Chinese biographical database |  |  |  | \n",
      "Topic 285: video | summarization | multimodal | videos | gplms          -> Abstractive Summarization for Open-Domain Videos |  |  |  | \n",
      "Topic 286: synonym | synonyms | synsets | synset | wordnet              -> Synonym-aware pre-training for semantic semantic understanding |  |  |  | \n",
      "Topic 287: chinese | characters | character | glyph | migbert           -> Glyce: Glyce-based Chinese Character Modeling |  |  |  | \n",
      "Topic 288: knowledge | lms | keplms | kbs | entities                    -> Language Models as Knowledge Bases |  |  |  | \n",
      "Topic 289: knnmt | datastore | nearest | neighbor | knearestneighbor    -> Fast $k$NN-MT |  |  |  | \n",
      "Topic 290: ppi | cdr | biomedical | proteinprotein | biocreative        -> Knowledge-aware attention model for chemical-disease relation extraction |  |  |  | \n",
      "Topic 291: memes | meme | memotion | mhameme | image                    -> Meme Generation and Generation |  |  |  | \n",
      "Topic 292: reviews | product | review | user | helpfulness              -> Hierarchical Fusion Attention Network for Sentiment Classification of User and Product Information |  |  |  | \n",
      "Topic 293: music | lyrics | musical | genres | genre                    -> Multilingual Music Genre Detection |  |  |  | \n",
      "Topic 294: entrainment | team | teamwork | discussions | teams          -> Entrainment in spoken dialogs |  |  |  | \n",
      "Topic 295: distributional | tensors | tensor | compositional | semantics -> Distributional Distributional Semantics |  |  |  | \n",
      "Topic 296: conversational | cqa | history | question | qr               -> Conversational Question Answering |  |  |  | \n",
      "Topic 297: ordering | pairwise | order | sentence | sentences           -> Sentence Ordering |  |  |  | \n",
      "Topic 298: preposition | prepositions | attachment | prepositional | supersense -> Prepositionsense |  |  |  | \n",
      "Topic 299: misinformation | covid19 | fake | cmta | pandemic            -> Covid-19: Fake News Detection |  |  |  | \n",
      "Topic 300: french | lexicongrammar | tables | lexicon | adverbs         -> lexicon grammar |  |  |  | \n",
      "Topic 301: climate | policy | change | climaterelated | planning        -> Climate change |  |  |  | \n",
      "Topic 302: parsing | sqar | semantic | wikisql | feedback               -> Syntactic Question Abstraction and Retrieval for Semantic Parsing |  |  |  | \n",
      "Topic 303: comments | comment | commenting | news | article             -> Automatic Commenting |  |  |  | \n",
      "Topic 304: deduplication | langid | corpora | koala | crawling          -> esCorpius: a Spanish corpus |  |  |  | \n",
      "Topic 305: comments | discussion | controversial | discussions | socialtext -> graphnli |  |  |  | \n",
      "Topic 306: hallucinations | hallucination | exposure | hallucinated | nmt -> Hallucinations in Neural Machine Translation |  |  |  | \n",
      "Topic 307: automata | finitestate | automaton | strings | multitape     -> TSSL Functions |  |  |  | \n",
      "Topic 308: wikipedia | article | articles | citations | references      -> Wikipedia Recommends Using WikiRef |  |  |  | \n",
      "Topic 309: podcast | podcasts | summary | audio | summaries             -> Podcast Summarization |  |  |  | \n",
      "Topic 310: prf | search | query | spoke | conversational                -> A reinforcement learning method for conversational search |  |  |  | \n",
      "Topic 311: counterfactual | counterfactuals | cda | perturbations | spurious -> Counterfactual Reasoning Model |  |  |  | \n",
      "Topic 312: sports | commentaries | game | esports | commentary          -> Sports game summarization |  |  |  | \n",
      "Topic 313: adversarial | discriminator | responses | response | dialogue -> A adversarial training approach for open-domain dialogue generation |  |  |  | \n",
      "Topic 314: triple | extraction | triples | relational | joint           -> Joint Entity and Relational Extraction |  |  |  | \n",
      "Topic 315: semisupervised | selftraining | pseudolabel | selfpretraining | cotraining -> SelfMix: A Self-Trained Neural Network for Text Classification |  |  |  | \n",
      "Topic 316: anomaly | anomalies | novelty | detection | outlier          -> Textual anomaly detection using deep neural networks |  |  |  | \n",
      "Topic 317: procedural | propara | tracking | actions | entity           -> Procedural text comprehension |  |  |  | \n",
      "Topic 318: itn | normalization | tn | inverse | errors                  -> Duplex Text Normalization for Text-to-Speech Synthesis and Automatic Speech Recognition |  |  |  | \n",
      "Topic 319: nlg | realization | surface | planning | naturalness         -> Natural Language Generation |  |  |  | \n",
      "Topic 320: cmr | decision | rule | conversational | reading             -> entailment reasoning in conversational machine reading |  |  |  | \n",
      "Topic 321: ssnmt | umt | multilingual | translation | mrasp2            -> Low-Resource Translation |  |  |  | \n",
      "Topic 322: diagrams | charts | diagrammatic | chart | diagram           -> Chart-to-text: A System for Chart Question Answering |  |  |  | \n",
      "Topic 323: adversarial | dadc | fool | adversarially | questions        -> Artificial Learning for Reading Comprehension |  |  |  | \n",
      "Topic 324: negotiation | persuasion | resisting | negotiate | tactics   -> Negotiation dialogue systems |  |  |  | \n",
      "Topic 325: hae | accidents | safety | accident | reports                -> A novel deep learning model for classification of hazard events |  |  |  | \n",
      "Topic 326: sense | wsi | senses | induction | dkpca                     -> AutoSense: A Novel Method for Learning Sensorial Induction |  |  |  | \n",
      "Topic 327: parallel | mwe | pairs | cac | million                       -> Multilingual and Bilingual Corpora for Natural Language Processing |  |  |  | \n",
      "Topic 328: nkb | kic | knowledge | factual | lama                       -> Knowledge-in-Context: A Novel Language Model Architecture |  |  |  | \n",
      "Topic 329: deception | deceptive | mafia | truthful | individualismcollectivism -> Deception detection in text across cultures |  |  |  | \n",
      "Topic 330: meaning | semantics | lattice | vector | entailment          -> Vector representations of language |  |  |  | \n",
      "Topic 331: parallel | smt | pivot | corpus | czeng                      -> POS tagging and rule-based approach for parallel translation of a text |  |  |  | \n",
      "Topic 332: relation | semeval2010 | shortest | networks | convolutional -> Deep Recurrent Neural Networks for Relation Classification |  |  |  | \n"
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
