{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Write an email apologizing to Sarah for the garden mishap. Explain how it happened.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 1.45MB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:01<00:00, 906kB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 1.78MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.03MB/s]\n",
      "model.safetensors: 100%|██████████| 548M/548M [03:37<00:00, 2.52MB/s] \n",
      "generation_config.json: 100%|██████████| 124/124 [00:00<00:00, 701kB/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"gpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write an email apologizing to Sarah for the garden mishap. Explain how it happened.\n",
      "\n",
      "\"I'm sorry for the inconvenience,\" she said. \"I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having to go through this. I'm sorry for the inconvenience of having\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the input prompt\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "# Generate the text\n",
    "generation_output = model.generate(\n",
    "  input_ids=input_ids,\n",
    "  max_new_tokens=256\n",
    ")\n",
    "# Print the output\n",
    "print(tokenizer.decode(generation_output[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
